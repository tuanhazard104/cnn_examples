{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f800720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8dd1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlockPytorch(nn.Module):\n",
    "  def __init__(self, num_channels, output_channels, strides=1, is_used_conv11=False, **kwargs):\n",
    "    \"\"\"\n",
    "    num_channels: số kênh\n",
    "    \"\"\"\n",
    "    super(ResidualBlockPytorch, self).__init__(**kwargs)\n",
    "    self.is_used_conv11 = is_used_conv11\n",
    "    self.conv1 = nn.Conv2d(num_channels, num_channels, padding=1, \n",
    "                           kernel_size=3, stride=1)\n",
    "    self.batch_norm = nn.BatchNorm2d(num_channels)\n",
    "    self.conv2 = nn.Conv2d(num_channels, num_channels, padding=1, \n",
    "                           kernel_size=3, stride=1)\n",
    "    if self.is_used_conv11:\n",
    "      self.conv3 = nn.Conv2d(num_channels, num_channels, padding=0, \n",
    "                           kernel_size=1, stride=1)\n",
    "    # Last convolutional layer to reduce output block shape.\n",
    "    self.conv4 = nn.Conv2d(num_channels, output_channels, padding=0, \n",
    "                           kernel_size=1, stride=strides)\n",
    "    self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "  def forward(self, X):\n",
    "    if self.is_used_conv11:\n",
    "      Y = self.conv3(X)\n",
    "    else:\n",
    "      Y = X\n",
    "    X = self.conv1(X)\n",
    "    X = self.relu(X)\n",
    "    X = self.batch_norm(X)\n",
    "    X = self.relu(X)\n",
    "    X = self.conv2(X)\n",
    "    X = self.batch_norm(X)\n",
    "    X = self.relu(X+Y)\n",
    "    X = self.conv4(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "732f6bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand((4, 1, 28, 28)) # shape=(batch_size, channels, width, height)\n",
    "X = ResidualBlockPytorch(num_channels=1, output_channels=64, strides=2, is_used_conv11=True)(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "908faa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18PyTorch(nn.Module):\n",
    "  def __init__(self, residual_blocks, output_shape):\n",
    "    super(ResNet18PyTorch, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, stride=2, padding=3)\n",
    "    self.batch_norm = nn.BatchNorm2d(64)\n",
    "    self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.residual_blocks = nn.Sequential(*residual_blocks)\n",
    "    self.global_avg_pool = nn.Flatten()\n",
    "    self.dense = nn.Linear(in_features=512, out_features=output_shape)\n",
    "\n",
    "  def forward(self, X):\n",
    "    X = self.conv1(X)\n",
    "    X = self.batch_norm(X)\n",
    "    X = self.relu(X)\n",
    "    X = self.max_pool(X)\n",
    "    X = self.residual_blocks(X)\n",
    "    X = self.global_avg_pool(X)\n",
    "    X = self.dense(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2ddfa44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 14, 14]           3,200\n",
      "       BatchNorm2d-2           [-1, 64, 14, 14]             128\n",
      "              ReLU-3           [-1, 64, 14, 14]               0\n",
      "         MaxPool2d-4             [-1, 64, 7, 7]               0\n",
      "            Conv2d-5             [-1, 64, 7, 7]          36,928\n",
      "              ReLU-6             [-1, 64, 7, 7]               0\n",
      "       BatchNorm2d-7             [-1, 64, 7, 7]             128\n",
      "              ReLU-8             [-1, 64, 7, 7]               0\n",
      "            Conv2d-9             [-1, 64, 7, 7]          36,928\n",
      "      BatchNorm2d-10             [-1, 64, 7, 7]             128\n",
      "             ReLU-11             [-1, 64, 7, 7]               0\n",
      "           Conv2d-12             [-1, 64, 4, 4]           4,160\n",
      "ResidualBlockPytorch-13             [-1, 64, 4, 4]               0\n",
      "           Conv2d-14             [-1, 64, 4, 4]          36,928\n",
      "             ReLU-15             [-1, 64, 4, 4]               0\n",
      "      BatchNorm2d-16             [-1, 64, 4, 4]             128\n",
      "             ReLU-17             [-1, 64, 4, 4]               0\n",
      "           Conv2d-18             [-1, 64, 4, 4]          36,928\n",
      "      BatchNorm2d-19             [-1, 64, 4, 4]             128\n",
      "             ReLU-20             [-1, 64, 4, 4]               0\n",
      "           Conv2d-21             [-1, 64, 2, 2]           4,160\n",
      "ResidualBlockPytorch-22             [-1, 64, 2, 2]               0\n",
      "           Conv2d-23             [-1, 64, 2, 2]           4,160\n",
      "           Conv2d-24             [-1, 64, 2, 2]          36,928\n",
      "             ReLU-25             [-1, 64, 2, 2]               0\n",
      "      BatchNorm2d-26             [-1, 64, 2, 2]             128\n",
      "             ReLU-27             [-1, 64, 2, 2]               0\n",
      "           Conv2d-28             [-1, 64, 2, 2]          36,928\n",
      "      BatchNorm2d-29             [-1, 64, 2, 2]             128\n",
      "             ReLU-30             [-1, 64, 2, 2]               0\n",
      "           Conv2d-31            [-1, 128, 1, 1]           8,320\n",
      "ResidualBlockPytorch-32            [-1, 128, 1, 1]               0\n",
      "           Conv2d-33            [-1, 128, 1, 1]         147,584\n",
      "             ReLU-34            [-1, 128, 1, 1]               0\n",
      "      BatchNorm2d-35            [-1, 128, 1, 1]             256\n",
      "             ReLU-36            [-1, 128, 1, 1]               0\n",
      "           Conv2d-37            [-1, 128, 1, 1]         147,584\n",
      "      BatchNorm2d-38            [-1, 128, 1, 1]             256\n",
      "             ReLU-39            [-1, 128, 1, 1]               0\n",
      "           Conv2d-40            [-1, 128, 1, 1]          16,512\n",
      "ResidualBlockPytorch-41            [-1, 128, 1, 1]               0\n",
      "           Conv2d-42            [-1, 128, 1, 1]          16,512\n",
      "           Conv2d-43            [-1, 128, 1, 1]         147,584\n",
      "             ReLU-44            [-1, 128, 1, 1]               0\n",
      "      BatchNorm2d-45            [-1, 128, 1, 1]             256\n",
      "             ReLU-46            [-1, 128, 1, 1]               0\n",
      "           Conv2d-47            [-1, 128, 1, 1]         147,584\n",
      "      BatchNorm2d-48            [-1, 128, 1, 1]             256\n",
      "             ReLU-49            [-1, 128, 1, 1]               0\n",
      "           Conv2d-50            [-1, 256, 1, 1]          33,024\n",
      "ResidualBlockPytorch-51            [-1, 256, 1, 1]               0\n",
      "           Conv2d-52            [-1, 256, 1, 1]         590,080\n",
      "             ReLU-53            [-1, 256, 1, 1]               0\n",
      "      BatchNorm2d-54            [-1, 256, 1, 1]             512\n",
      "             ReLU-55            [-1, 256, 1, 1]               0\n",
      "           Conv2d-56            [-1, 256, 1, 1]         590,080\n",
      "      BatchNorm2d-57            [-1, 256, 1, 1]             512\n",
      "             ReLU-58            [-1, 256, 1, 1]               0\n",
      "           Conv2d-59            [-1, 256, 1, 1]          65,792\n",
      "ResidualBlockPytorch-60            [-1, 256, 1, 1]               0\n",
      "           Conv2d-61            [-1, 256, 1, 1]          65,792\n",
      "           Conv2d-62            [-1, 256, 1, 1]         590,080\n",
      "             ReLU-63            [-1, 256, 1, 1]               0\n",
      "      BatchNorm2d-64            [-1, 256, 1, 1]             512\n",
      "             ReLU-65            [-1, 256, 1, 1]               0\n",
      "           Conv2d-66            [-1, 256, 1, 1]         590,080\n",
      "      BatchNorm2d-67            [-1, 256, 1, 1]             512\n",
      "             ReLU-68            [-1, 256, 1, 1]               0\n",
      "           Conv2d-69            [-1, 512, 1, 1]         131,584\n",
      "ResidualBlockPytorch-70            [-1, 512, 1, 1]               0\n",
      "           Conv2d-71            [-1, 512, 1, 1]       2,359,808\n",
      "             ReLU-72            [-1, 512, 1, 1]               0\n",
      "      BatchNorm2d-73            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-74            [-1, 512, 1, 1]               0\n",
      "           Conv2d-75            [-1, 512, 1, 1]       2,359,808\n",
      "      BatchNorm2d-76            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-77            [-1, 512, 1, 1]               0\n",
      "           Conv2d-78            [-1, 512, 1, 1]         262,656\n",
      "ResidualBlockPytorch-79            [-1, 512, 1, 1]               0\n",
      "          Flatten-80                  [-1, 512]               0\n",
      "           Linear-81                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 8,518,858\n",
      "Trainable params: 8,518,858\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.67\n",
      "Params size (MB): 32.50\n",
      "Estimated Total Size (MB): 33.17\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "residual_blocks = [\n",
    "    # Two start conv mapping\n",
    "    ResidualBlockPytorch(num_channels=64, output_channels=64, strides=2, is_used_conv11=False),\n",
    "    ResidualBlockPytorch(num_channels=64, output_channels=64, strides=2, is_used_conv11=False),\n",
    "    # Next three [conv mapping + identity mapping]\n",
    "    ResidualBlockPytorch(num_channels=64, output_channels=128, strides=2, is_used_conv11=True),\n",
    "    ResidualBlockPytorch(num_channels=128, output_channels=128, strides=2, is_used_conv11=False),\n",
    "    ResidualBlockPytorch(num_channels=128, output_channels=256, strides=2, is_used_conv11=True),\n",
    "    ResidualBlockPytorch(num_channels=256, output_channels=256, strides=2, is_used_conv11=False),\n",
    "    ResidualBlockPytorch(num_channels=256, output_channels=512, strides=2, is_used_conv11=True),\n",
    "    ResidualBlockPytorch(num_channels=512, output_channels=512, strides=2, is_used_conv11=False)\n",
    "]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ptmodel = ResNet18PyTorch(residual_blocks, output_shape=10)\n",
    "ptmodel.to(device)\n",
    "summary(ptmodel, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca3658c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c0ff8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.05), (0.05))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
    "                                          shuffle=True, num_workers=8)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
    "                                         shuffle=False, num_workers=8)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ptmodel.parameters(), lr=0.001, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cca419c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(output, label):\n",
    "    # output: (batch, num_output) float32 ndarray\n",
    "    # label: (batch, ) int32 ndarray\n",
    "    return (torch.argmax(output, axis=1)==label).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e6f64cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 500: loss 0.816, train acc 0.764 in 209.2 sec\n",
      "iter 1000: loss 0.584, train acc 0.841 in 192.3 sec\n",
      "iter 1500: loss 0.470, train acc 0.877 in 185.3 sec\n",
      "Epoch 0: loss 0.416, train acc 0.892, test acc 0.972, in 745.2 sec\n",
      "iter 500: loss 0.164, train acc 0.968 in 190.4 sec\n",
      "iter 1000: loss 0.158, train acc 0.968 in 185.9 sec\n",
      "iter 1500: loss 0.151, train acc 0.970 in 190.2 sec\n",
      "Epoch 1: loss 0.146, train acc 0.970, test acc 0.979, in 717.1 sec\n",
      "iter 500: loss 0.125, train acc 0.975 in 188.6 sec\n",
      "iter 1000: loss 0.119, train acc 0.977 in 183.8 sec\n",
      "iter 1500: loss 0.115, train acc 0.977 in 187.0 sec\n",
      "Epoch 2: loss 0.113, train acc 0.977, test acc 0.984, in 713.0 sec\n",
      "iter 500: loss 0.086, train acc 0.986 in 202.3 sec\n",
      "iter 1000: loss 0.088, train acc 0.983 in 192.1 sec\n",
      "iter 1500: loss 0.088, train acc 0.983 in 191.0 sec\n",
      "Epoch 3: loss 0.088, train acc 0.982, test acc 0.983, in 740.1 sec\n",
      "iter 500: loss 0.075, train acc 0.987 in 187.9 sec\n",
      "iter 1000: loss 0.078, train acc 0.985 in 185.6 sec\n",
      "iter 1500: loss 0.077, train acc 0.985 in 187.1 sec\n",
      "Epoch 4: loss 0.077, train acc 0.985, test acc 0.987, in 712.6 sec\n",
      "iter 500: loss 0.076, train acc 0.987 in 188.6 sec\n",
      "iter 1000: loss 0.069, train acc 0.987 in 187.4 sec\n",
      "iter 1500: loss 0.069, train acc 0.987 in 186.1 sec\n",
      "Epoch 5: loss 0.067, train acc 0.987, test acc 0.982, in 715.1 sec\n",
      "iter 500: loss 0.060, train acc 0.989 in 189.2 sec\n",
      "iter 1000: loss 0.059, train acc 0.989 in 187.1 sec\n",
      "iter 1500: loss 0.061, train acc 0.989 in 187.4 sec\n",
      "Epoch 6: loss 0.064, train acc 0.987, test acc 0.983, in 716.6 sec\n",
      "iter 500: loss 0.058, train acc 0.991 in 189.8 sec\n",
      "iter 1000: loss 0.061, train acc 0.989 in 186.7 sec\n",
      "iter 1500: loss 0.061, train acc 0.989 in 194.6 sec\n",
      "Epoch 7: loss 0.061, train acc 0.988, test acc 0.988, in 725.6 sec\n",
      "iter 500: loss 0.053, train acc 0.991 in 190.7 sec\n",
      "iter 1000: loss 0.055, train acc 0.990 in 186.2 sec\n",
      "iter 1500: loss 0.055, train acc 0.990 in 186.9 sec\n",
      "Epoch 8: loss 0.054, train acc 0.989, test acc 0.986, in 716.7 sec\n",
      "iter 500: loss 0.049, train acc 0.992 in 189.7 sec\n",
      "iter 1000: loss 0.051, train acc 0.990 in 187.3 sec\n",
      "iter 1500: loss 0.051, train acc 0.990 in 187.2 sec\n",
      "Epoch 9: loss 0.050, train acc 0.990, test acc 0.989, in 717.6 sec\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    total_loss = 0.0\n",
    "    tic = time.time()\n",
    "    tic_step = time.time()\n",
    "    train_acc = 0.0\n",
    "    valid_acc = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = ptmodel(inputs)\n",
    "        train_acc += acc(outputs, labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        total_loss += loss.item()\n",
    "        if i % 500 == 499:\n",
    "          print(\"iter %d: loss %.3f, train acc %.3f in %.1f sec\" % (\n",
    "            i+1, total_loss/i, train_acc/i, time.time()-tic_step))\n",
    "          tic_step = time.time()\n",
    "\n",
    "    # calculate validation accuracy\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        valid_acc += acc(ptmodel(inputs), labels)\n",
    "\n",
    "    print(\"Epoch %d: loss %.3f, train acc %.3f, test acc %.3f, in %.1f sec\" % (\n",
    "            epoch, total_loss/len(trainloader), train_acc/len(trainloader),\n",
    "            valid_acc/len(testloader), time.time()-tic))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477a2469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
